{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
       ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
    "!pip3 install lxml requests numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crawling\n",
    "\n",
    "Let's start by downloading some relevant documents. For this challenge we will be using the medical guidelines published by the UK [National Institute for Health and Care Excellence (NICE)](https://www.nice.org.uk/), specifically the [Clinical Knowledge Summaries (CKS)](https://cks.nice.org.uk/).\n",
    "\n",
    "Start by having a look at the website to familiarise yourself with the content. \n",
    "\n",
    "\n",
    ">**IMPORTANT:** The NICE CKS are only served to IP ranges from the UK and its territories, so to access this content from outside the UK you will need [a VPN](https://www.google.com/search?q=free%20vpn).\n",
    "\n",
    "**Task:**\n",
    "- identify and download every single guideline page from NICE CKS. Save it as plain HTML in the `data/cks/` folder. Here is one example guideline: https://cks.nice.org.uk/acute-kidney-injury\n",
    "- extra bonus points if you make sure to only download files that were last modified at a later date than the last version of the file in the `data/` folder\n",
    "\n",
    "*Tips:*\n",
    "- Although when rendered by a web browser the users needs to click on each section to see it, in fact each of these guidelines is served as a single HTML page which makes them perfect for this exercise.\n",
    "- in order to download them all, you need to find links to every single page\n",
    "- the front CKS page is an AJAX site: it is dynamically rendered. The whole list of all guidelines is actually stored on the server as a text file and retrieved by the client to render. Identifying and retrieving this file would make your task much easier. Chrome dev tools is your friend :)\n",
    "- make sure to save everything locally from the start. Restarting and redownloading is bad crawling practice: it may trigger some blocking!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, abspath\n",
    "\n",
    "import requests # recommended library to use for crawling, but feel free to use any other\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "CKS_DIR = abspath(join(DATA_DIR,'cks'))\n",
    "\n",
    "def get_cks_urls():\n",
    "    ROOT_URL = 'https://cks.nice.org.uk/'\n",
    "    \n",
    "    urls = []\n",
    "    # your code here\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def crawl_cks(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True) # create the data directory if it doesn't exist\n",
    "    urls = get_cks_urls()\n",
    "    successful_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        text = \"\"\n",
    "        filename = \"\"\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        if success:\n",
    "            with open(filename, 'w', encoding='utf8') as f:\n",
    "                f.write(text)\n",
    "        else:\n",
    "            # your code here\n",
    "            print(error_message)\n",
    "    \n",
    "    return successful_urls\n",
    "    \n",
    "crawl_cks(CKS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping\n",
    "\n",
    "Each paragraph (or first-level bullet-point) on these pages roughtly equates to one possible answer to a clinician's query, so we will treat them as single paragraphs. \n",
    "\n",
    "**Your task is to:**\n",
    "- extract the document title\n",
    "- extract the description meta tag, if present\n",
    "- extract last revision date\n",
    "- split the document into smaller chunks that we'll call \"paragraphs\"\n",
    "    - each first-level bullet point under &lt;section&gt; counts as one paragraph of text, including the text contents of all of its children elements. We need to extract the text from all HTML elements under the &lt;li&gt; and group it as one paragraph\n",
    "    - each top-evel &lt;p&gt; under &lt;section&gt; is also a paragraph\n",
    "- clean the text. For the purposes of this challenge, we want to remove all inline HTML tags but preserve the text.\n",
    "\n",
    "**Tips**\n",
    "- You will see that even after all this processing and cleaning, the data is still a mess. For example, in the CKS format, the lowest level of headers is rendered as &lt;p&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;/p&gt;. This is a headache for building scrapers but for this exercise it doesn't matter much: we are not trying to retrieve perfectly formatted text, just to clean it a bit before indexing and to group it into vaguely useful chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following utility methods are provided for convenience and needed for the scraping below\n",
    "from lxml import html\n",
    "\n",
    "IGNORE_SECTIONS = ['references',\n",
    "                   'how this topic was developed',\n",
    "                   'how up-to-date is this topic?',\n",
    "                   'supporting evidence']\n",
    "\n",
    "def stringify_children(node):\n",
    "    \"Returns the full HTML code of an elment in the parse tree as a string\"\n",
    "    from lxml.etree import tostring\n",
    "    from itertools import chain\n",
    "    import html\n",
    "\n",
    "    parts = ([node.text] +\n",
    "             list(chain(*([c.text, tostring(c).decode(\"utf-8\"), c.tail] for c in node.getchildren()))) +\n",
    "             [node.tail])\n",
    "    text = ''.join(filter(None, parts))\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    text = text.replace('\\u00a0', ' ')\n",
    "    text = text.replace('\\u00a3', 'Â£')\n",
    "    text = clean_text(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def join_headers(paragraphs, min_len=150):\n",
    "    \"\"\" Joins orphan header lines into the next paragraph. \n",
    "        A header is any line below a length of `min_len` \"\"\"\n",
    "    changes = True\n",
    "    while changes:\n",
    "        changes = False\n",
    "\n",
    "        for index in range(len(paragraphs) - 1):\n",
    "            if len(paragraphs[index]) < min_len:\n",
    "                new_par = paragraphs[index] + ': ' + paragraphs[index + 1]\n",
    "                paragraphs = paragraphs[:index] + [new_par] + paragraphs[index + 2:]\n",
    "                changes=True\n",
    "                break\n",
    "    return paragraphs\n",
    "\n",
    "def yield_paragraphs(doc):\n",
    "    \"A generator to export all paragraphs from a document\"\n",
    "    for section in doc.get('sections', []):\n",
    "        for para in section.get('paragraphs', []):\n",
    "            yield para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for some regular expressions! I recommend https://regex101.com/ for live checking the \n",
    "# results of your code\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove links and HTML formatting code from a string and normalise the text\"\"\"\n",
    "    \n",
    "    text = # your code here: remove all <a> tags that are bibliography references, including the text between the <a></a> tags\n",
    "    \n",
    "    text = # your code here: remove all text inside <strong> tags \n",
    "    text = # your code here: replace each list item tag with the string ' - '\n",
    "\n",
    "    text = # your code here: remove all remaining HTML tags, but not the text between them\n",
    "    \n",
    "    text = re.sub('\\[\\s*\\.?\\s*\\[', '[', text)\n",
    "    text = re.sub('\\]\\s*\\.?\\s*\\]', ']', text)\n",
    "\n",
    "    text = # your code here: remove all text in between square brackets, e.g '[ some text ]'\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def scrape_cks_section(section):\n",
    "    paragraphs = []\n",
    "    title_nodes = section.xpath('header/*[self::h1 or self::h2 or self::h3 or self::h4]')\n",
    "    if len(title_nodes) > 0:\n",
    "        sec_title = title_nodes[0].text\n",
    "    else:\n",
    "        sec_title = ''\n",
    "\n",
    "    # your code here: if the section title appears in the IGNORE_SECTIONS list, return None\n",
    "    \n",
    "    for item in section.xpath('*[self::p or self::ul or self::section]'):\n",
    "        if item.tag == 'ul':\n",
    "            # your code here\n",
    "        elif item.tag == 'p':\n",
    "            # your code here\n",
    "        elif item.tag == 'section':\n",
    "            sec_dict = scrape_cks_section(item)\n",
    "\n",
    "            if sec_dict is None or len(sec_dict.get('paragraphs', [])) == 0:\n",
    "                continue\n",
    "\n",
    "            # your code here: if the section has a title, add the title as its own paragraph\n",
    "            # use paragraphs.append()\n",
    "            \n",
    "            # your code here: add all paragraphs in the section to the `paragraphs` list\n",
    "\n",
    "    res = {\n",
    "        'title': sec_title,\n",
    "        'paragraphs': join_headers(paragraphs)\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "def scrape_cks(text):\n",
    "    tree = html.fromstring(text)\n",
    "    doc_title = # your code here: get the document title from inside <article> \n",
    "    doc_description = # your code here: get the document description from the <head>\n",
    "    last_revision = # your code here: get the text for last_revision inside <article> \n",
    "\n",
    "    sections = []\n",
    "    \n",
    "    for section in tree.xpath('YOUR-CODE-HERE'): # an xpath selector to iterate through all <section> tags\n",
    "        new_section = scrape_cks_section(section)\n",
    "        if new_section:\n",
    "            sections.append(new_section)\n",
    "\n",
    "    doc = {\n",
    "        'title': doc_title,\n",
    "        'sections': sections\n",
    "    }\n",
    "    return doc\n",
    "\n",
    "def scrape_cks_files(cks_dir):\n",
    "    filenames = [join(cks_dir, f) for f in listdir(cks_dir) if\n",
    "                 isfile(join(cks_dir, f)) and os.path.splitext(join(cks_dir, f))[1].startswith('.htm')]\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        with open(filename, 'r') as f:\n",
    "            text = f.read()\n",
    "        doc = scrape_cks(text)\n",
    "        doc['filename'] = # your code here: the name of the source file, without the directory\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = scrape_cks_files(CKS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing\n",
    "\n",
    "Once we have extracted and cleaned the text, we can now create useful representations of it for retrieval / question answering. There are infinitely ways of doing this, but for this exercise, we are going to use dense vector representations generated by Google's deep learning USE v4 model.\n",
    "\n",
    "This is a near-state-of-the-art model that generates multilingual sentence embeddings, that is, turns a sequence of characters in a natural language into a multidimensional vector of real numbers. In theory, the closer the meaning of the sentences, the closer these vectors of real numbers will be in the 512-dimensional space.\n",
    "\n",
    "In this model, the dimensions of the vector representing the document don't hold any specific meaning, they are all real numbers and the vector is of a predefined size: it does not grow with the growth of the vocabulary.\n",
    "\n",
    "Our approach will be to embed each paragraph and each query with USE and find the nearest answer to each question by exhaustively computing the inner product of the query vector and the paragraph vector.\n",
    "\n",
    "**Task:**\n",
    "- generate an index of the documents by:\n",
    "    - creating a large list of all the paragraphs in all the documents: `all_paragraphs`\n",
    "    - compute the embedding vector for each paragraph: `vectors`\n",
    "    - keep a list of dictionaries containing metadata for each paragraph: `doc_info`, including 'filename' (the original filename) and 'doc_par_num' (paragraph number inside the document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to set up the environment\n",
    "!pip3 install --upgrade tensorflow\n",
    "!pip3 install  tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://tfhub.dev/google/universal-sentence-encoder/4 for details\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url) # this will take a good while the first time, it's a 1 gb download\n",
    "print (\"module %s loaded\" % module_url)\n",
    "\n",
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(docs):\n",
    "    \"\"\" \n",
    "    This function should return 3 lists, of the same length (total number of paragraphs):\n",
    "        - doc_info: a list of dictionaries, one for each paragraph. \n",
    "           Each dict should at least contain the keys \n",
    "           - 'filename': the source file that the paragraph comes from\n",
    "           - 'doc_par_num': the index of that paragraph inside the list of paragraphs \n",
    "           inside the doc dictionary (i.e. for each doc in docs)\n",
    "              \n",
    "        - all_paragraphs: a list of all paragraphs, flattened from each doc\n",
    "        \n",
    "        - vectors: an array/list of vectors, where each vector is generated using the \n",
    "        USE model loaded above \n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    return doc_info, all_paragraphs, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_info, all_paragraphs, vectors = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval / Question Answering\n",
    "\n",
    "We are now going to try to use our index to answer a list of clinical questions from [a family medicine website](https://www.mdedge.com/familymedicine/clinical-inquiries). This is a way to informally evaluate how well we can answer these questions by simply retrieving the most relevant paragraph found in our collection.\n",
    "\n",
    "We will compute the similarity using a very naive approach: the distance in space between the vectors generated by USE for the query and each paragraph.\n",
    "\n",
    "**Task:**\n",
    "- load questions from '../data/ebm_questions.txt'\n",
    "- for each question, retrieve the best match from the collection of paragraphs\n",
    "- output results to a CSV \n",
    "    - CSV should have these columns: question, answer, score, filename\n",
    "    - results should be sorted by score in decreasing order (i.e. question with highest answer score first)\n",
    "    - filename is the name of the CKS file you have saved locally\n",
    "    - score is the inner product or cosine distance between the query and document vectors\n",
    "- bonus points for exporting columns *in this order*: question, answer, score, filename\n",
    "- bonus points for any suggestion on how to improve on this extremely naive approach\n",
    "    \n",
    "**Tips:**\n",
    "- the easiest way to compute the similarity/score between query and document is the inner product. Numpy is your friend. So is Google :)\n",
    "- if you find that the answers coming out are terrible, don't worry, this is expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_FILE = '../data/ebm_questions.txt'\n",
    "\n",
    "questions = # your code here: load the list of questions into this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieval(query):\n",
    "    q_vector = # your code here: generate vector for the query\n",
    "    \n",
    "    scores = # your code here: compute the scores between the query and every paragraph in `vectors`\n",
    "    docs = # your code here: create a list of tuples where the first element is a counter from \n",
    "           # 0 to len(scores)-1, and the second element is the corresponding element at that \n",
    "           # position in the scores array\n",
    "    results = # your code here: sort the elements in the `docs` list by their score, \n",
    "              # in reverse order\n",
    "    \n",
    "    top_result = {\n",
    "        'filename': doc_info[results[0][0]]['filename'], \n",
    "        'score': # your code here,\n",
    "        'answer': # your code here,\n",
    "    }\n",
    "\n",
    "    return top_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # you may find it useful for CSV export\n",
    "\n",
    "def answer_questions(questions):\n",
    "    results=[]\n",
    "    for q in questions:\n",
    "        answer = retrieval(q)\n",
    "        answer['question'] = q\n",
    "        \n",
    "        results.append(answer)\n",
    "    return results\n",
    "\n",
    "def results_to_csv(results, filename):\n",
    "    # your code: sort results by score, descending \n",
    "    # your code: export results to csv file `filename`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute answers and export to a CSV\n",
    "results_to_csv(answer_questions(questions), 'answers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submitting results\n",
    "\n",
    
    
    "**Task:**  Please save, zip and send this `.ipynb` file and the `answers.csv` file (if you have generated it) to [dan@medwise.ai](mailto:dan@medwise.ai). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
